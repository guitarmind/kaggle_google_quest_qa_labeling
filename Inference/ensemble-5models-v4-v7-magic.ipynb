{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling pytorch-transformers-1.1.0:\r\n",
      "  Successfully uninstalled pytorch-transformers-1.1.0\r\n"
     ]
    }
   ],
   "source": [
    "# # For Kernal Mode\n",
    "!pip install -q ../input/tensorflow-determinism\n",
    "!pip install -q ../input/huggingfacetokenizers/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl\n",
    "# !pip install -q ../input/sacremoses\n",
    "!pip uninstall --yes pytorch-transformers\n",
    "!pip install -q ../input/huggingface-transformers-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.1.0\n",
      "PyTorch version 1.3.0\n",
      "Transformers version 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import random, math, time\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import bisect\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "# https://github.com/NVIDIA/tensorflow-determinism\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1' # TF 2.1\n",
    "# from tfdeterminism import patch\n",
    "# patch()\n",
    "\n",
    "import transformers\n",
    "from transformers import *\n",
    "\n",
    "import torch\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print('Tensorflow version', tf.__version__)\n",
    "\n",
    "print('PyTorch version', torch.__version__)\n",
    "\n",
    "print('Transformers version',\n",
    "      transformers.__version__)  # Current version: 2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['huggingface-transformers-master',\n",
       " 'huggingfacetokenizers',\n",
       " 'google-quest-challenge',\n",
       " 'tf-roberta-base-exp-v7',\n",
       " 'tf-roberta-base',\n",
       " 'xlnet-base-cased-exp-v7',\n",
       " 'bert-base-uncased',\n",
       " 'xlnet-base-cased',\n",
       " 'tf-bert-base-cased',\n",
       " 'tf-roberta-base-exp-v4',\n",
       " 'bert-base-uncased-exp-v4',\n",
       " 'tf-bert-base-cased-exp-v4',\n",
       " 'tensorflow-determinism']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 20201120\n",
    "n_splits = 5\n",
    "\n",
    "# BERT_PATH = \"/workspace/Kaggle/QA/pretrained_models/\"\n",
    "# dataset_folder = Path(\"/workspace/Kaggle/QA/\")\n",
    "# MODEL_PATH_list = [\n",
    "#     \"/workspace/Kaggle/QA/completed/tf-roberta-base-exp-v7/\",\n",
    "#     \"/workspace/Kaggle/QA/completed/bert-base-uncased-exp-v4/\",\n",
    "#     \"/workspace/Kaggle/QA/completed/tf-bert-base-cased-exp-v4/\",\n",
    "#     \"/workspace/Kaggle/QA/completed/tf-roberta-base-exp-v4/\",\n",
    "#     \"/workspace/Kaggle/QA/completed/xlnet-base-cased-exp-v7/\",\n",
    "# ]\n",
    "\n",
    "BERT_PATH = \"../input/\"\n",
    "dataset_folder = Path(\"../input/google-quest-challenge\")\n",
    "MODEL_PATH_list = [\n",
    "    \"../input/tf-roberta-base-exp-v7/\",\n",
    "    \"../input/bert-base-uncased-exp-v4/\",\n",
    "    \"../input/tf-bert-base-cased-exp-v4/\",\n",
    "    \"../input/tf-roberta-base-exp-v4/\",\n",
    "    \"../input/xlnet-base-cased-exp-v7/\",\n",
    "]\n",
    "\n",
    "# pretrained_model_metadata = [\n",
    "#     # (pretrained_model_name, is_tf, infer_batch_size, cate_embed_mode)\n",
    "#     (\"tf-roberta-base\", True, 32, True),\n",
    "#     (\"bert-base-uncased\", True, 32, False),\n",
    "#     (\"tf-bert-base-cased\", True, 32, False),\n",
    "#     (\"tf-roberta-base\", True, 32, False),\n",
    "#     (\"xlnet-base-cased\", True, 32, True),\n",
    "# ]\n",
    "\n",
    "pretrained_model_metadata = [\n",
    "    # (pretrained_model_name, is_tf, infer_batch_size, cate_embed_mode)\n",
    "    (\"tf-roberta-base\", True, 56, True),\n",
    "    (\"bert-base-uncased\", True, 56, False),\n",
    "    (\"tf-bert-base-cased\", True, 56, False),\n",
    "    (\"tf-roberta-base\", True, 56, False),\n",
    "    (\"xlnet-base-cased\", True, 48, True),\n",
    "]\n",
    "\n",
    "model_filename_prefix_list = [\n",
    "    \"tf-roberta-base_exp_cate_embed\",\n",
    "    \"bert-base-uncased_exp_split_dense\",\n",
    "    \"tf-bert-base-cased_exp_split_dense\",\n",
    "    \"tf-roberta-base_exp_split_dense\",\n",
    "    \"xlnet-base-cased_exp_cate_embed\",\n",
    "]\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "max_title_length = 100\n",
    "\n",
    "# learning_rate = 2e-5\n",
    "# embeddings_dropout = 0.2\n",
    "# dense_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/tf-roberta-base-exp-v7/tf-roberta-base_exp_cate_embed_fold3.h5\n",
      "../input/tf-roberta-base-exp-v7/tf-roberta-base_exp_cate_embed_fold2.h5\n",
      "../input/tf-roberta-base-exp-v7/tf-roberta-base_exp_cate_embed_fold1.h5\n",
      "../input/tf-roberta-base-exp-v7/tf-roberta-base_exp_cate_embed_fold5.h5\n",
      "../input/tf-roberta-base-exp-v7/tf-roberta-base_exp_cate_embed_fold4.h5\n",
      "../input/bert-base-uncased-exp-v4/bert-base-uncased_exp_split_dense_fold4.h5\n",
      "../input/bert-base-uncased-exp-v4/bert-base-uncased_exp_split_dense_fold5.h5\n",
      "../input/bert-base-uncased-exp-v4/bert-base-uncased_exp_split_dense_fold3.h5\n",
      "../input/bert-base-uncased-exp-v4/bert-base-uncased_exp_split_dense_fold1.h5\n",
      "../input/bert-base-uncased-exp-v4/bert-base-uncased_exp_split_dense_fold2.h5\n",
      "../input/tf-bert-base-cased-exp-v4/tf-bert-base-cased_exp_split_dense_fold1.h5\n",
      "../input/tf-bert-base-cased-exp-v4/tf-bert-base-cased_exp_split_dense_fold4.h5\n",
      "../input/tf-bert-base-cased-exp-v4/tf-bert-base-cased_exp_split_dense_fold3.h5\n",
      "../input/tf-bert-base-cased-exp-v4/tf-bert-base-cased_exp_split_dense_fold5.h5\n",
      "../input/tf-bert-base-cased-exp-v4/tf-bert-base-cased_exp_split_dense_fold2.h5\n",
      "../input/tf-roberta-base-exp-v4/tf-roberta-base_exp_split_dense_fold2.h5\n",
      "../input/tf-roberta-base-exp-v4/tf-roberta-base_exp_split_dense_fold1.h5\n",
      "../input/tf-roberta-base-exp-v4/tf-roberta-base_exp_split_dense_fold3.h5\n",
      "../input/tf-roberta-base-exp-v4/tf-roberta-base_exp_split_dense_fold5.h5\n",
      "../input/tf-roberta-base-exp-v4/tf-roberta-base_exp_split_dense_fold4.h5\n",
      "../input/xlnet-base-cased-exp-v7/xlnet-base-cased_exp_cate_embed_fold2.h5\n",
      "../input/xlnet-base-cased-exp-v7/xlnet-base-cased_exp_cate_embed_fold4.h5\n",
      "../input/xlnet-base-cased-exp-v7/xlnet-base-cased_exp_cate_embed_fold1.h5\n",
      "../input/xlnet-base-cased-exp-v7/xlnet-base-cased_exp_cate_embed_fold3.h5\n",
      "../input/xlnet-base-cased-exp-v7/xlnet-base-cased_exp_cate_embed_fold5.h5\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(MODEL_PATH_list):\n",
    "    prefix = model_filename_prefix_list[i]\n",
    "    for f in os.listdir(p):\n",
    "        if f != \"dataset-metadata.json\":\n",
    "            print(p+f)\n",
    "            assert prefix in f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6079, 41)\n",
      "Test shape: (476, 11)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(dataset_folder / 'train.csv')\n",
    "df_test = pd.read_csv(dataset_folder / 'test.csv')\n",
    "df_sub = pd.read_csv(dataset_folder / 'sample_submission.csv')\n",
    "print('Train shape:', df_train.shape)\n",
    "print('Test shape:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output categories:\n",
      " ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
      "\n",
      "Input categories:\n",
      " ['question_title', 'question_body', 'answer']\n"
     ]
    }
   ],
   "source": [
    "output_categories = list(df_train.columns[11:])\n",
    "# Select only question title, body and answer\n",
    "input_categories = list(df_train.columns[[1, 2, 5]])\n",
    "\n",
    "print('\\nOutput categories:\\n', output_categories)\n",
    "print('\\nInput categories:\\n', input_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract domain\n",
    "def extract_netloc(x):\n",
    "    tokens = x.split(\".\")\n",
    "    if len(tokens) > 3:\n",
    "        print(x)\n",
    "        return \".\".join(tokens[:2])\n",
    "        # looks like meta is a special site, we should keep it\n",
    "        # https://stackoverflow.com/help/whats-meta\n",
    "        # the part of the site where users discuss the workings and policies of Stack Overflow rather than discussing programming itself.\n",
    "        # return tokens[1]\n",
    "    else:\n",
    "        return tokens[0]\n",
    "\n",
    "\n",
    "# TODO: test it\n",
    "# df_train['netloc'] = df_train['host'].apply(\n",
    "#     lambda x: extract_netloc(x))\n",
    "# df_test['netloc'] = df_test['host'].apply(\n",
    "#     lambda x: extract_netloc(x))\n",
    "\n",
    "df_train['netloc'] = df_train['host'].apply(lambda x: x.split(\".\")[0])\n",
    "df_test['netloc'] = df_test['host'].apply(lambda x: x.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(rand_seed):\n",
    "    np.random.seed(rand_seed)\n",
    "    random.seed(rand_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(rand_seed)\n",
    "    \n",
    "    # TF 2.0\n",
    "    tf.random.set_seed(rand_seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(rand_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirect outputs to console\n",
    "# import sys\n",
    "# jupyter_console = sys.stdout\n",
    "# sys.stdout = open('/dev/stdout', 'w')\n",
    "\n",
    "# Append to log file\n",
    "# sys.stdout = open(f\"stdout.log\", 'a')\n",
    "# sys.stdout = jupyter_console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer,\n",
    "                                   max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "        inputs = tokenizer.encode_plus(str1,\n",
    "                                       str2,\n",
    "                                       add_special_tokens=True,\n",
    "                                       max_length=length,\n",
    "                                       truncation_strategy=truncation_strategy)\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "\n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    def remove_html_special_symbols(x):\n",
    "        html_entities = [\n",
    "            (\"&quot;\", \"\\\"\"),\n",
    "            (\"&num;\", \"#\"),\n",
    "            (\"&dollar;\", \"$\"),\n",
    "            (\"&percnt;\", \"%\"),\n",
    "            (\"&amp;\", \"&\"),\n",
    "            (\"&apos;\", \"'\"),\n",
    "            (\"&lpar;\", \"(\"),\n",
    "            (\"&rpar;\", \")\"),\n",
    "            (\"&ast;\", \"*\"),\n",
    "            (\"&plus;\", \"+\"),\n",
    "            (\"&comma;\", \",\"),\n",
    "            (\"&minus;\", \"-\"),\n",
    "            (\"&period;\", \".\"),\n",
    "            (\"&sol;\", \"/\"),\n",
    "            (\"&colon;\", \":\"),\n",
    "            (\"&semi;\", \";\"),\n",
    "            (\"&lt;\", \"<\"),\n",
    "            (\"&equals;\", \"=\"),\n",
    "            (\"&gt;\", \">\"),\n",
    "            (\"&quest;\", \"?\"),\n",
    "            (\"&commat;\", \"@\"),\n",
    "            (\"&lsqb;\", \"[\"),\n",
    "            (\"&bsol;\", \"\\\\\"),\n",
    "            (\"&rsqb;\", \"]\"),\n",
    "            (\"&Hat;\", \"^\"),\n",
    "            (\"&lowbar;\", \"_\"),\n",
    "            (\"&grave;\", \"`\"),\n",
    "            (\"&lcub;\", \"{\"),\n",
    "            (\"&verbar;\", \"|\"),\n",
    "            (\"&rcub;\", \"}\"),\n",
    "            # (\"\", \"\"),\n",
    "        ]\n",
    "        for (k, v) in html_entities:\n",
    "            x = str(x.replace(k, v))\n",
    "        return x\n",
    "\n",
    "    def remove_latex_and_code_tokens(tokens):\n",
    "        return [\n",
    "            x for x in tokens if not (x.startswith(\"$\") or x.startswith(\"\\\\\"))\n",
    "        ]\n",
    "\n",
    "    # Remove extra spaces\n",
    "    title = remove_html_special_symbols(\" \".join(\n",
    "        remove_latex_and_code_tokens(str(title).split()))).strip()\n",
    "    question = remove_html_special_symbols(\" \".join(\n",
    "        remove_latex_and_code_tokens(str(question).split()))).strip()\n",
    "    answer = remove_html_special_symbols(\" \".join(\n",
    "        remove_latex_and_code_tokens(str(answer).split()))).strip()\n",
    "\n",
    "    # Extract plain text from html\n",
    "    try:\n",
    "        soup_q = BeautifulSoup(question)\n",
    "        question = soup_q.get_text()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        soup_a = BeautifulSoup(answer)\n",
    "        answer = soup_a.get_text()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        \"[CLS] \" + title[:max_title_length] + \" [SEP] \" + question + \" [SEP]\", None,\n",
    "        'longest_first', max_sequence_length)\n",
    "\n",
    "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "        \"[CLS] \" + answer + \" [SEP]\", None, 'longest_first', max_sequence_length)\n",
    "\n",
    "    return [\n",
    "        input_ids_q, input_masks_q, input_segments_q, input_ids_a,\n",
    "        input_masks_a, input_segments_a\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "\n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "        input_ids_a.append(ids_a)\n",
    "        input_masks_a.append(masks_a)\n",
    "        input_segments_a.append(segments_a)\n",
    "\n",
    "    return [\n",
    "        np.asarray(input_ids_q, dtype=np.int32),\n",
    "        np.asarray(input_masks_q, dtype=np.int32),\n",
    "        np.asarray(input_segments_q, dtype=np.int32),\n",
    "        np.asarray(input_ids_a, dtype=np.int32),\n",
    "        np.asarray(input_masks_a, dtype=np.int32),\n",
    "        np.asarray(input_segments_a, dtype=np.int32)\n",
    "    ]\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)\n",
    "\n",
    "def compute_spearmanr(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.mean(rhos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpearmanMonitorCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, valid_data, batch_size=16, fold=None):\n",
    "        self.valid_inputs = valid_data[0]\n",
    "        self.valid_outputs = valid_data[1]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valid_predictions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.valid_predictions.append(\n",
    "            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n",
    "\n",
    "        rho_val = compute_spearmanr(\n",
    "            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n",
    "\n",
    "        print(f\" Fold {self.fold+1} Validation Score: {rho_val:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(pretrained_model_name):\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "\n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "\n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "\n",
    "    pretrained_model = model_class.from_pretrained(BERT_PATH +\n",
    "                                                   f\"{pretrained_model_name}\")\n",
    "\n",
    "    q_embedding = pretrained_model(q_id,\n",
    "                                   attention_mask=q_mask,\n",
    "                                   token_type_ids=q_atn)[0]\n",
    "    a_embedding = pretrained_model(a_id,\n",
    "                                   attention_mask=a_mask,\n",
    "                                   token_type_ids=a_atn)[0]\n",
    "\n",
    "#     q_embedding = tf.keras.layers.SpatialDropout1D(embeddings_dropout)(\n",
    "#         q_embedding)\n",
    "#     a_embedding = tf.keras.layers.SpatialDropout1D(embeddings_dropout)(\n",
    "#         a_embedding)\n",
    "\n",
    "\n",
    "    # Get CLS token output\n",
    "    q = q_embedding[:, 0, :]\n",
    "    a = a_embedding[:, 0, :]\n",
    "\n",
    "    q = tf.keras.layers.Dense(256, activation='relu')(q)\n",
    "    a = tf.keras.layers.Dense(256, activation='relu')(a)\n",
    "    \n",
    "    # TODO: Test dense dropout\n",
    "    # q = tf.keras.layers.Dropout(dense_dropout)(q)\n",
    "    # a = tf.keras.layers.Dropout(dense_dropout)(a)\n",
    "\n",
    "    q = tf.keras.layers.Dense(21, activation='sigmoid')(q)\n",
    "    a = tf.keras.layers.Dense(9, activation='sigmoid')(a)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([q, a])\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[\n",
    "        q_id,\n",
    "        q_mask,\n",
    "        q_atn,\n",
    "        a_id,\n",
    "        a_mask,\n",
    "        a_atn,\n",
    "    ],\n",
    "                                  outputs=x)\n",
    "\n",
    "    return model, pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_cate_embed(pretrained_model_name, embed_info):\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "\n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "\n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "\n",
    "    pretrained_model = model_class.from_pretrained(BERT_PATH +\n",
    "                                                   f\"{pretrained_model_name}\")\n",
    "\n",
    "    # Get last hidden-state from 1st element of output\n",
    "    if \"xlnet\" in pretrained_model_name:\n",
    "        q_embedding = pretrained_model(q_id,\n",
    "                                       attention_mask=q_mask,\n",
    "                                       token_type_ids=q_atn)[0]\n",
    "        a_embedding = pretrained_model(a_id,\n",
    "                                       attention_mask=a_mask,\n",
    "                                       token_type_ids=a_atn)[0]\n",
    "    else:\n",
    "        q_embedding, q_pooler_output = pretrained_model(q_id,\n",
    "                                                        attention_mask=q_mask,\n",
    "                                                        token_type_ids=q_atn)\n",
    "        a_embedding, a_pooler_output = pretrained_model(a_id,\n",
    "                                                        attention_mask=a_mask,\n",
    "                                                        token_type_ids=a_atn)\n",
    "\n",
    "    # Get CLS token output\n",
    "    q = q_embedding[:, 0, :]\n",
    "    a = a_embedding[:, 0, :]\n",
    "\n",
    "    host_input = tf.keras.Input(shape=(1, ), name=\"host_input\")\n",
    "    netloc_input = tf.keras.Input(shape=(1, ), name=\"netloc_input\")\n",
    "    cate_input = tf.keras.Input(shape=(1, ), name=\"category_input\")\n",
    "\n",
    "    host_embed_info = embed_info[\"host\"]\n",
    "    host_embed = tf.keras.layers.Embedding(input_dim=host_embed_info[0],\n",
    "                                           output_dim=host_embed_info[1],\n",
    "                                           input_length=(1, ))(host_input)\n",
    "\n",
    "    netloc_embed_info = embed_info[\"netloc\"]\n",
    "    netloc_embed = tf.keras.layers.Embedding(input_dim=netloc_embed_info[0],\n",
    "                                             output_dim=netloc_embed_info[1],\n",
    "                                             input_length=(1, ))(netloc_input)\n",
    "\n",
    "    cate_embed_info = embed_info[\"category\"]\n",
    "    cate_embed = tf.keras.layers.Embedding(input_dim=cate_embed_info[0],\n",
    "                                           output_dim=cate_embed_info[1],\n",
    "                                           input_length=(1, ))(cate_input)\n",
    "\n",
    "    host_embed = tf.keras.layers.Reshape(\n",
    "        target_shape=(host_embed_info[1], ))(host_embed)\n",
    "    netloc_embed = tf.keras.layers.Reshape(\n",
    "        target_shape=(netloc_embed_info[1], ))(netloc_embed)\n",
    "    cate_embed = tf.keras.layers.Reshape(\n",
    "        target_shape=(cate_embed_info[1], ))(cate_embed)\n",
    "\n",
    "    embed_concat = tf.keras.layers.Concatenate()(\n",
    "        [host_embed, netloc_embed, cate_embed])\n",
    "    embed_concat = tf.keras.layers.Dense(128, activation='relu')(embed_concat)\n",
    "\n",
    "    # Concatenation\n",
    "    q_concat = tf.keras.layers.Concatenate()([q, embed_concat])\n",
    "    # q_concat = tf.keras.layers.Concatenate()([q, host_embed, cate_embed, q_pooler_output])\n",
    "    q_concat = tf.keras.layers.Dense(256, activation='relu')(q_concat)\n",
    "\n",
    "    a_concat = tf.keras.layers.Concatenate()([a, embed_concat])\n",
    "    # a_concat = tf.keras.layers.Concatenate()([a, host_embed, cate_embed, a_pooler_output])\n",
    "    a_concat = tf.keras.layers.Dense(256, activation='relu')(a_concat)\n",
    "\n",
    "    # Dense dropout\n",
    "    # q_concat = tf.keras.layers.Dropout(dense_dropout)(q_concat)\n",
    "    # a_concat = tf.keras.layers.Dropout(dense_dropout)(a_concat)\n",
    "\n",
    "    # Use sigmoid for multi-label predictions\n",
    "    q_concat = tf.keras.layers.Dense(21, activation='sigmoid')(q_concat)\n",
    "    a_concat = tf.keras.layers.Dense(9, activation='sigmoid')(a_concat)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([q_concat, a_concat])\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[\n",
    "        q_id, q_mask, q_atn, a_id, a_mask, a_atn, host_input, netloc_input,\n",
    "        cate_input\n",
    "    ],\n",
    "                                  outputs=x)\n",
    "\n",
    "    return model, pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split K-Folds by Unique Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_all_seeds(rand_seed)\n",
    "gkf = GroupKFold(n_splits=n_splits).split(X=df_train.question_body,\n",
    "                                          groups=df_train.question_body)\n",
    "gkf = list(gkf)\n",
    "len(gkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = compute_output_arrays(df_train, output_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ranks(preds, unique_labels):\n",
    "    new_preds = np.zeros(preds.shape)\n",
    "    for i in range(preds.shape[1]):\n",
    "        interpolate_bins = np.digitize(preds[:, i],\n",
    "                                       bins=unique_labels,\n",
    "                                       right=False)\n",
    "        \n",
    "        if len(np.unique(interpolate_bins)) == 1:\n",
    "            # Use original preds\n",
    "            new_preds[:, i] = preds[:, i]\n",
    "        else:\n",
    "            new_preds[:, i] = unique_labels[interpolate_bins]\n",
    "\n",
    "    return new_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.2       , 0.26666667, 0.3       , 0.33333333,\n",
       "       0.33333333, 0.4       , 0.44444444, 0.46666667, 0.5       ,\n",
       "       0.53333333, 0.55555556, 0.6       , 0.66666667, 0.66666667,\n",
       "       0.7       , 0.73333333, 0.77777778, 0.8       , 0.83333333,\n",
       "       0.86666667, 0.88888889, 0.9       , 0.93333333, 1.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels = df_train[output_categories].copy()\n",
    "y_labels = y_labels.values.flatten()\n",
    "unique_labels = np.array(sorted(np.unique(y_labels)))\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.08      , 0.16      , 0.21333333, 0.24      ,\n",
       "       0.26666667, 0.28      , 0.29333333, 0.30666667, 0.32      ,\n",
       "       0.33333333, 0.33333333, 0.33333333, 0.34666667, 0.37333333,\n",
       "       0.4       , 0.41777778, 0.43555556, 0.44888889, 0.45777778,\n",
       "       0.46666667, 0.48      , 0.49333333, 0.50666667, 0.52      ,\n",
       "       0.53333333, 0.54222222, 0.55111111, 0.56444444, 0.58222222,\n",
       "       0.6       , 0.62666667, 0.65333333, 0.66666667, 0.66666667,\n",
       "       0.66666667, 0.68      , 0.69333333, 0.70666667, 0.72      ,\n",
       "       0.73333333, 0.75111111, 0.76888889, 0.78222222, 0.79111111,\n",
       "       0.8       , 0.81333333, 0.82666667, 0.84      , 0.85333333,\n",
       "       0.86666667, 0.87555556, 0.88444444, 0.89111111, 0.89555556,\n",
       "       0.9       , 0.91333333, 0.92666667, 0.94666667, 0.97333333,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denominator = 60\n",
    "q = np.arange(0, 101, 100 / denominator)\n",
    "exp_labels = np.percentile(unique_labels, q)\n",
    "exp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction from fine-tuned tf-roberta-base ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6079it [00:32, 186.33it/s]\n",
      "476it [00:02, 184.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on preprocessing: 0.65 minutes\n",
      "Using tf-roberta-base Fold 1 tf-roberta-base for inference ......\n",
      "Extracted (vocab_size, embedding_size) for host: (64, 32)\n",
      "Extracted (vocab_size, embedding_size) for netloc: (60, 30)\n",
      "Extracted (vocab_size, embedding_size) for category: (6, 3)\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 1 Validation Score: 0.401543\n",
      "Fold 1 Magic Validation Score: 0.420954\n",
      "Time spent on fold 1 inference: 1.39 minutes\n",
      "Using tf-roberta-base Fold 2 tf-roberta-base for inference ......\n",
      "Extracted (vocab_size, embedding_size) for host: (63, 31)\n",
      "Extracted (vocab_size, embedding_size) for netloc: (60, 30)\n",
      "Extracted (vocab_size, embedding_size) for category: (6, 3)\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 2 Validation Score: 0.394542\n",
      "Fold 2 Magic Validation Score: 0.420386\n",
      "Time spent on fold 2 inference: 1.34 minutes\n",
      "Using tf-roberta-base Fold 3 tf-roberta-base for inference ......\n",
      "Extracted (vocab_size, embedding_size) for host: (62, 31)\n",
      "Extracted (vocab_size, embedding_size) for netloc: (59, 29)\n",
      "Extracted (vocab_size, embedding_size) for category: (6, 3)\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 3 Validation Score: 0.404243\n",
      "Fold 3 Magic Validation Score: 0.425357\n",
      "Time spent on fold 3 inference: 1.34 minutes\n",
      "Using tf-roberta-base Fold 4 tf-roberta-base for inference ......\n",
      "Extracted (vocab_size, embedding_size) for host: (62, 31)\n",
      "Extracted (vocab_size, embedding_size) for netloc: (60, 30)\n",
      "Extracted (vocab_size, embedding_size) for category: (6, 3)\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 4 Validation Score: 0.387427\n",
      "Fold 4 Magic Validation Score: 0.405359\n",
      "Time spent on fold 4 inference: 1.34 minutes\n",
      "Using tf-roberta-base Fold 5 tf-roberta-base for inference ......\n",
      "Extracted (vocab_size, embedding_size) for host: (64, 32)\n",
      "Extracted (vocab_size, embedding_size) for netloc: (60, 30)\n",
      "Extracted (vocab_size, embedding_size) for category: (6, 3)\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 5 Validation Score: 0.392107\n",
      "Fold 5 Magic Validation Score: 0.417903\n",
      "Time spent on fold 5 inference: 1.35 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:00, 85.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction from fine-tuned bert-base-uncased ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6079it [01:24, 72.00it/s]\n",
      "476it [00:07, 65.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on preprocessing: 1.59 minutes\n",
      "Using bert-base-uncased Fold 1 bert-base-uncased for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 1 Validation Score: 0.391418\n",
      "Fold 1 Magic Validation Score: 0.404021\n",
      "Time spent on fold 1 inference: 1.33 minutes\n",
      "Using bert-base-uncased Fold 2 bert-base-uncased for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 2 Validation Score: 0.386496\n",
      "Fold 2 Magic Validation Score: 0.409046\n",
      "Time spent on fold 2 inference: 1.31 minutes\n",
      "Using bert-base-uncased Fold 3 bert-base-uncased for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 3 Validation Score: 0.396813\n",
      "Fold 3 Magic Validation Score: 0.407842\n",
      "Time spent on fold 3 inference: 1.32 minutes\n",
      "Using bert-base-uncased Fold 4 bert-base-uncased for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 4 Validation Score: 0.386607\n",
      "Fold 4 Magic Validation Score: 0.397898\n",
      "Time spent on fold 4 inference: 1.32 minutes\n",
      "Using bert-base-uncased Fold 5 bert-base-uncased for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 5 Validation Score: 0.383731\n",
      "Fold 5 Magic Validation Score: 0.401169\n",
      "Time spent on fold 5 inference: 1.31 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 114.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction from fine-tuned tf-bert-base-cased ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6079it [01:10, 85.78it/s]\n",
      "476it [00:05, 79.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on preprocessing: 1.35 minutes\n",
      "Using tf-bert-base-cased Fold 1 tf-bert-base-cased for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 1 Validation Score: 0.393316\n",
      "Fold 1 Magic Validation Score: 0.412791\n",
      "Time spent on fold 1 inference: 1.33 minutes\n",
      "Using tf-bert-base-cased Fold 2 tf-bert-base-cased for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 2 Validation Score: 0.382069\n",
      "Fold 2 Magic Validation Score: 0.409234\n",
      "Time spent on fold 2 inference: 1.31 minutes\n",
      "Using tf-bert-base-cased Fold 3 tf-bert-base-cased for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 3 Validation Score: 0.396037\n",
      "Fold 3 Magic Validation Score: 0.413046\n",
      "Time spent on fold 3 inference: 1.32 minutes\n",
      "Using tf-bert-base-cased Fold 4 tf-bert-base-cased for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 4 Validation Score: 0.383596\n",
      "Fold 4 Magic Validation Score: 0.395466\n",
      "Time spent on fold 4 inference: 1.32 minutes\n",
      "Using tf-bert-base-cased Fold 5 tf-bert-base-cased for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 5 Validation Score: 0.380183\n",
      "Fold 5 Magic Validation Score: 0.395866\n",
      "Time spent on fold 5 inference: 1.31 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 119.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction from fine-tuned tf-roberta-base ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6079it [00:32, 187.67it/s]\n",
      "476it [00:02, 192.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on preprocessing: 0.65 minutes\n",
      "Using tf-roberta-base Fold 1 tf-roberta-base for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 1 Validation Score: 0.402144\n",
      "Fold 1 Magic Validation Score: 0.425709\n",
      "Time spent on fold 1 inference: 1.34 minutes\n",
      "Using tf-roberta-base Fold 2 tf-roberta-base for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 2 Validation Score: 0.395371\n",
      "Fold 2 Magic Validation Score: 0.420505\n",
      "Time spent on fold 2 inference: 1.34 minutes\n",
      "Using tf-roberta-base Fold 3 tf-roberta-base for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 3 Validation Score: 0.408052\n",
      "Fold 3 Magic Validation Score: 0.433750\n",
      "Time spent on fold 3 inference: 1.34 minutes\n",
      "Using tf-roberta-base Fold 4 tf-roberta-base for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 4 Validation Score: 0.392775\n",
      "Fold 4 Magic Validation Score: 0.415862\n",
      "Time spent on fold 4 inference: 1.33 minutes\n",
      "Using tf-roberta-base Fold 5 tf-roberta-base for inference ......\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 5 Validation Score: 0.394978\n",
      "Fold 5 Magic Validation Score: 0.416440\n",
      "Time spent on fold 5 inference: 1.33 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:00, 221.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating prediction from fine-tuned xlnet-base-cased ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6079it [00:27, 219.61it/s]\n",
      "476it [00:02, 230.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on preprocessing: 0.56 minutes\n",
      "Using xlnet-base-cased Fold 1 xlnet-base-cased for inference ......\n",
      "Extracted (vocab_size, embedding_size) for host: (64, 32)\n",
      "Extracted (vocab_size, embedding_size) for netloc: (60, 30)\n",
      "Extracted (vocab_size, embedding_size) for category: (6, 3)\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 1 Validation Score: 0.399029\n",
      "Fold 1 Magic Validation Score: 0.414248\n",
      "Time spent on fold 1 inference: 2.92 minutes\n",
      "Using xlnet-base-cased Fold 2 xlnet-base-cased for inference ......\n",
      "Extracted (vocab_size, embedding_size) for host: (63, 31)\n",
      "Extracted (vocab_size, embedding_size) for netloc: (60, 30)\n",
      "Extracted (vocab_size, embedding_size) for category: (6, 3)\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 2 Validation Score: 0.388116\n",
      "Fold 2 Magic Validation Score: 0.415639\n",
      "Time spent on fold 2 inference: 2.78 minutes\n",
      "Using xlnet-base-cased Fold 3 xlnet-base-cased for inference ......\n",
      "Extracted (vocab_size, embedding_size) for host: (62, 31)\n",
      "Extracted (vocab_size, embedding_size) for netloc: (59, 29)\n",
      "Extracted (vocab_size, embedding_size) for category: (6, 3)\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 3 Validation Score: 0.399847\n",
      "Fold 3 Magic Validation Score: 0.425905\n",
      "Time spent on fold 3 inference: 2.79 minutes\n",
      "Using xlnet-base-cased Fold 4 xlnet-base-cased for inference ......\n",
      "Extracted (vocab_size, embedding_size) for host: (62, 31)\n",
      "Extracted (vocab_size, embedding_size) for netloc: (60, 30)\n",
      "Extracted (vocab_size, embedding_size) for category: (6, 3)\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 4 Validation Score: 0.389818\n",
      "Fold 4 Magic Validation Score: 0.405260\n",
      "Time spent on fold 4 inference: 2.79 minutes\n",
      "Using xlnet-base-cased Fold 5 xlnet-base-cased for inference ......\n",
      "Extracted (vocab_size, embedding_size) for host: (64, 32)\n",
      "Extracted (vocab_size, embedding_size) for netloc: (60, 30)\n",
      "Extracted (vocab_size, embedding_size) for category: (6, 3)\n",
      "Cleaning session ...\n",
      "Loading pretrained model and weights ...\n",
      "Fold 5 Validation Score: 0.386459\n",
      "Fold 5 Magic Validation Score: 0.403277\n",
      "Time spent on fold 5 inference: 2.80 minutes\n",
      "Time spent on ensemble inference: 45.66 minutes\n"
     ]
    }
   ],
   "source": [
    "infer_start_time = time.time()\n",
    "\n",
    "all_test_preds = []\n",
    "all_val_preds = []\n",
    "all_val_scores = []\n",
    "all_magic_val_scores = []\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "for k, MODEL_PATH in enumerate(MODEL_PATH_list):\n",
    "    \n",
    "    pretrained_model_name, is_tf, infer_batch_size, cate_embed_mode = pretrained_model_metadata[k]\n",
    "    model_filename_prefix = model_filename_prefix_list[k]\n",
    "\n",
    "    print(f\"Generating prediction from fine-tuned {pretrained_model_name} ......\")\n",
    "    \n",
    "    preprocessing_start = time.time()\n",
    "    if is_tf:\n",
    "        model_class = TFAutoModel\n",
    "        tokenizer_class = AutoTokenizer\n",
    "    else:\n",
    "        model_class = AutoModel\n",
    "        tokenizer_class = AutoTokenizer\n",
    "    tokenizer = tokenizer_class.from_pretrained(BERT_PATH +\n",
    "                                                f\"{pretrained_model_name}\")\n",
    "    inputs = compute_input_arrays(df_train, input_categories, tokenizer,\n",
    "                                  MAX_SEQUENCE_LENGTH)\n",
    "    test_inputs = compute_input_arrays(df_test, input_categories,\n",
    "                                       tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "    print(f\"Time spent on preprocessing: {(time.time()-preprocessing_start)/60:,.2f} minutes\")\n",
    "\n",
    "    test_preds = []\n",
    "    val_preds = []\n",
    "    val_scores = []\n",
    "    magic_val_scores = []\n",
    "\n",
    "    for i, (train_idx, valid_idx) in enumerate(gkf):\n",
    "        set_all_seeds(rand_seed)\n",
    "\n",
    "        print(f\"Using {pretrained_model_name} Fold {i+1} {pretrained_model_name} for inference ......\")\n",
    "\n",
    "        fold_start = time.time()\n",
    "\n",
    "        # Generate validation score\n",
    "        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "        valid_outputs = outputs[valid_idx]\n",
    "        \n",
    "        if cate_embed_mode:\n",
    "            # Extra categorical embeddings\n",
    "            embed_info = {}\n",
    "            category_features = {}\n",
    "            def extract_category_ids(train, val, test, c, info):\n",
    "                le = LabelEncoder()\n",
    "                le.fit(train[c])\n",
    "                # Set unknonwn category\n",
    "                val[c] = val[c].map(lambda s: '<unknown>'\n",
    "                                    if s not in le.classes_ else s)\n",
    "                test[c] = test[c].map(lambda s: '<unknown>'\n",
    "                                      if s not in le.classes_ else s)\n",
    "\n",
    "                le_classes = le.classes_.tolist()\n",
    "                bisect.insort_left(le_classes, '<unknown>')\n",
    "                le.classes_ = le_classes\n",
    "\n",
    "                train[c + \"_label\"] = le.transform(train[c])\n",
    "                val[c + \"_label\"] = le.transform(val[c])\n",
    "                test[c + \"_label\"] = le.transform(test[c])\n",
    "\n",
    "                no_of_unique_cat = train[c + \"_label\"].nunique()\n",
    "                embedding_size = min(np.ceil((no_of_unique_cat) / 2), 50)\n",
    "                embedding_size = int(embedding_size)\n",
    "                vocab_size = no_of_unique_cat + 1\n",
    "                info[c] = (vocab_size, embedding_size)\n",
    "\n",
    "                print(f\"Extracted (vocab_size, embedding_size) for {c}: ({vocab_size}, {embedding_size})\")\n",
    "\n",
    "                return val[c + \"_label\"], test[c + \"_label\"]\n",
    "\n",
    "            host_val, host_test = extract_category_ids(df_train.iloc[train_idx, :].copy(),\n",
    "                                                       df_train.iloc[valid_idx, :].copy(),\n",
    "                                                       df_test.copy(),\n",
    "                                                       \"host\",\n",
    "                                                       embed_info)\n",
    "            netloc_val, netloc_test = extract_category_ids(df_train.iloc[train_idx, :].copy(),\n",
    "                                                           df_train.iloc[valid_idx, :].copy(),\n",
    "                                                           df_test.copy(),\n",
    "                                                           \"netloc\",\n",
    "                                                           embed_info)\n",
    "            cate_val, cate_test = extract_category_ids(df_train.iloc[train_idx, :].copy(),\n",
    "                                                       df_train.iloc[valid_idx, :].copy(),\n",
    "                                                       df_test.copy(),\n",
    "                                                       \"category\",\n",
    "                                                       embed_info)\n",
    "\n",
    "            valid_inputs.append(host_val)\n",
    "            valid_inputs.append(netloc_val)\n",
    "            valid_inputs.append(cate_val)\n",
    "\n",
    "            # Copy test_inputs\n",
    "            submit_inputs = [np.copy(x) for x in test_inputs]\n",
    "            submit_inputs.append(host_test)\n",
    "            submit_inputs.append(netloc_test)\n",
    "            submit_inputs.append(cate_test)\n",
    "        \n",
    "        print(\"Cleaning session ...\")\n",
    "        K.clear_session()    \n",
    "\n",
    "        print(\"Loading pretrained model and weights ...\")\n",
    "        if cate_embed_mode:\n",
    "            model, pretrained_model = create_model_cate_embed(pretrained_model_name, embed_info)\n",
    "        else:\n",
    "            model, pretrained_model = create_model(pretrained_model_name)\n",
    "            \n",
    "        model_filename = f\"{model_filename_prefix}_fold{i+1}.h5\"\n",
    "        # Load fine-tuned weights\n",
    "        model.load_weights(MODEL_PATH + model_filename)\n",
    "        \n",
    "        fold_val_preds = model.predict(valid_inputs,\n",
    "                                       batch_size=infer_batch_size)\n",
    "        rho_val = compute_spearmanr(valid_outputs, fold_val_preds)\n",
    "        print(f\"Fold {i+1} Validation Score: {rho_val:.6f}\")\n",
    "        val_preds.append(fold_val_preds)\n",
    "        val_scores.append(rho_val)\n",
    "        \n",
    "        val_magic_preds = optimize_ranks(fold_val_preds, exp_labels)\n",
    "        magic_rho_val = compute_spearmanr(valid_outputs, val_magic_preds)\n",
    "        print(f\"Fold {i+1} Magic Validation Score: {magic_rho_val:.6f}\")\n",
    "        magic_val_scores.append(magic_rho_val)\n",
    "\n",
    "        # Generate test predictions\n",
    "        if cate_embed_mode:\n",
    "            test_preds.append(model.predict(submit_inputs,\n",
    "                                            batch_size=infer_batch_size))\n",
    "        else:\n",
    "            test_preds.append(model.predict(test_inputs,\n",
    "                                            batch_size=infer_batch_size))\n",
    "\n",
    "        print(f\"Time spent on fold {i+1} inference: {(time.time()-fold_start)/60:,.2f} minutes\")\n",
    "\n",
    "        del model, pretrained_model, valid_inputs, valid_outputs, fold_val_preds, rho_val\n",
    "        gc.collect()\n",
    "\n",
    "    all_test_preds.append(test_preds)\n",
    "    all_val_preds.append(val_preds)\n",
    "    all_val_scores.append(val_scores)\n",
    "    all_magic_val_scores.append(magic_val_scores)\n",
    "    \n",
    "    del tokenizer, inputs, test_inputs\n",
    "    gc.collect()\n",
    "    \n",
    "print(f\"Time spent on ensemble inference: {(time.time()-infer_start_time)/60:,.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Validation Score: 0.392669\n",
      "Mean Magic Validation Score: 0.412517\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Validation Score: {np.mean(all_val_scores):.6f}\")\n",
    "print(f\"Mean Magic Validation Score: {np.mean(all_magic_val_scores):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Ensemble Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_ensemble_preds(all_val_preds, weights):\n",
    "    oof_preds = np.zeros(outputs.shape)\n",
    "    for i, model_preds in enumerate(all_val_preds):\n",
    "        for j, (train_idx, valid_idx) in enumerate(gkf):\n",
    "            tmp = np.vstack(model_preds[j])\n",
    "            oof_preds[valid_idx] += tmp * weights[i]\n",
    "\n",
    "    oof_preds /= np.sum(weights)\n",
    "\n",
    "    return oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ensemble-models-v4-v7.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_val_preds, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0] 0.437365294011373\n",
      "[2.0, 1.0, 1.0, 1.0, 2.0] 0.4381585386566243\n",
      "[2.0, 1.0, 1.0, 1.0, 1.5] 0.4382315353642368\n",
      "[1.5, 1.0, 1.0, 1.0, 1.5] 0.4377316599924199\n"
     ]
    }
   ],
   "source": [
    "weights = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "oof_preds = val_ensemble_preds(all_val_preds, weights)\n",
    "magic_preds = optimize_ranks(oof_preds, exp_labels)\n",
    "blend_score = compute_spearmanr(outputs, magic_preds)\n",
    "print(weights, blend_score)\n",
    "\n",
    "weights = [2.0, 1.0, 1.0, 1.0, 2.0]\n",
    "oof_preds = val_ensemble_preds(all_val_preds, weights)\n",
    "magic_preds = optimize_ranks(oof_preds, exp_labels)\n",
    "blend_score = compute_spearmanr(outputs, magic_preds)\n",
    "print(weights, blend_score)\n",
    "\n",
    "weights = [2.0, 1.0, 1.0, 1.0, 1.5]\n",
    "oof_preds = val_ensemble_preds(all_val_preds, weights)\n",
    "magic_preds = optimize_ranks(oof_preds, exp_labels)\n",
    "blend_score = compute_spearmanr(outputs, magic_preds)\n",
    "print(weights, blend_score)\n",
    "\n",
    "weights = [1.5, 1.0, 1.0, 1.0, 1.5]\n",
    "oof_preds = val_ensemble_preds(all_val_preds, weights)\n",
    "magic_preds = optimize_ranks(oof_preds, exp_labels)\n",
    "blend_score = compute_spearmanr(outputs, magic_preds)\n",
    "print(weights, blend_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_preds = [np.average(x, axis=0) for x in all_test_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_preds = np.average(submit_preds, weights = [2.0, 1.0, 1.0, 1.0, 1.5],\n",
    "                          axis=0)  # for weighted average set weights=[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize ranks\n",
    "submit_preds = optimize_ranks(submit_preds, exp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.iloc[:, 1:] = submit_preds\n",
    "df_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
