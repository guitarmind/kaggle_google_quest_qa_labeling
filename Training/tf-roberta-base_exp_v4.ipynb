{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kernal Mode\n",
    "\n",
    "# !pip install -q ../input/tensorflow-determinism\n",
    "# !pip install -q ../input/huggingfacetokenizers/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl\n",
    "# !pip uninstall --yes pytorch-transformers\n",
    "# !pip install -q ../input/huggingface-transformers-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Use only title (100) + question (206) + answer (206) (failed)\n",
    "2. LR decay factor=0.3 (failed)\n",
    "3. Use one embedding input instead of two (failed)\n",
    "4. Use three embedding inputs instead of two (failed)\n",
    "5. Split question and anwer FC layers\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import random, math, time\n",
    "import os, sys, re\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "# https://github.com/NVIDIA/tensorflow-determinism\n",
    "# os.environ['TF_DETERMINISTIC_OPS'] = '1' # TF 2.1\n",
    "from tfdeterminism import patch\n",
    "patch()\n",
    "\n",
    "import transformers\n",
    "from transformers import *\n",
    "\n",
    "import torch\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print('Tensorflow version', tf.__version__)\n",
    "\n",
    "print('PyTorch version', torch.__version__)\n",
    "\n",
    "print('Transformers version',\n",
    "      transformers.__version__)  # Current version: 2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_mode = True\n",
    "debug_mode = False\n",
    "\n",
    "kernel_mode = False\n",
    "# kernel_mode = True\n",
    "\n",
    "rand_seed = 20201120\n",
    "n_splits = 5\n",
    "\n",
    "dataset_folder = Path(\"/workspace/Kaggle/QA/\")\n",
    "BERT_PATH = \"/workspace/Kaggle/QA/pretrained_models/\"\n",
    "\n",
    "# dataset_folder = Path(\"../input/google-quest-challenge/\")\n",
    "# BERT_PATH = \"../input/huggingface-transformers/\"\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "# max_title_length = 50\n",
    "max_title_length = 100\n",
    "\n",
    "learning_rate = 2e-5\n",
    "embeddings_dropout = 0.05\n",
    "dense_dropout = 0.05\n",
    "# learning_rate = 2e-5\n",
    "# embeddings_dropout = 0.2\n",
    "# dense_dropout = 0.1\n",
    "\n",
    "if debug_mode:\n",
    "#     epochs = 2\n",
    "#     batch_size = 2\n",
    "    epochs = 15\n",
    "    batch_size = 2\n",
    "else:\n",
    "#     epochs = 6\n",
    "    epochs = 15\n",
    "    if kernel_mode:\n",
    "        batch_size = 4\n",
    "    else:\n",
    "        batch_size = 3\n",
    "#         batch_size = 4\n",
    "\n",
    "lr_decay_patience = 1\n",
    "early_stopping_patience = 2\n",
    "\n",
    "# lr_decay_patience = 2\n",
    "# early_stopping_patience = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(dataset_folder / 'train.csv')\n",
    "df_test = pd.read_csv(dataset_folder / 'test.csv')\n",
    "df_sub = pd.read_csv(dataset_folder / 'sample_submission.csv')\n",
    "print('Train shape:', df_train.shape)\n",
    "print('Test shape:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_categories = list(df_train.columns[11:])\n",
    "# Select only question title, body and answer\n",
    "input_categories = list(df_train.columns[[1, 2, 5]])\n",
    "\n",
    "print('\\nOutput categories:\\n', output_categories)\n",
    "print('\\nInput categories:\\n', input_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(rand_seed):\n",
    "    np.random.seed(rand_seed)\n",
    "    random.seed(rand_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(rand_seed)\n",
    "    \n",
    "    # TF 2.0\n",
    "    tf.random.set_seed(rand_seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(rand_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirect outputs to console\n",
    "import sys\n",
    "jupyter_console = sys.stdout\n",
    "sys.stdout = open('/dev/stdout', 'w')\n",
    "\n",
    "# Append to log file\n",
    "# sys.stdout = open(f\"stdout.log\", 'a')\n",
    "# sys.stdout = jupyter_console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer,\n",
    "                                   max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "        inputs = tokenizer.encode_plus(str1,\n",
    "                                       str2,\n",
    "                                       add_special_tokens=True,\n",
    "                                       max_length=length,\n",
    "                                       truncation_strategy=truncation_strategy)\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "\n",
    "        return [input_ids, input_masks, input_segments]\n",
    "\n",
    "    def remove_html_special_symbols(x):\n",
    "        html_entities = [\n",
    "            (\"&quot;\", \"\\\"\"),\n",
    "            (\"&num;\", \"#\"),\n",
    "            (\"&dollar;\", \"$\"),\n",
    "            (\"&percnt;\", \"%\"),\n",
    "            (\"&amp;\", \"&\"),\n",
    "            (\"&apos;\", \"'\"),\n",
    "            (\"&lpar;\", \"(\"),\n",
    "            (\"&rpar;\", \")\"),\n",
    "            (\"&ast;\", \"*\"),\n",
    "            (\"&plus;\", \"+\"),\n",
    "            (\"&comma;\", \",\"),\n",
    "            (\"&minus;\", \"-\"),\n",
    "            (\"&period;\", \".\"),\n",
    "            (\"&sol;\", \"/\"),\n",
    "            (\"&colon;\", \":\"),\n",
    "            (\"&semi;\", \";\"),\n",
    "            (\"&lt;\", \"<\"),\n",
    "            (\"&equals;\", \"=\"),\n",
    "            (\"&gt;\", \">\"),\n",
    "            (\"&quest;\", \"?\"),\n",
    "            (\"&commat;\", \"@\"),\n",
    "            (\"&lsqb;\", \"[\"),\n",
    "            (\"&bsol;\", \"\\\\\"),\n",
    "            (\"&rsqb;\", \"]\"),\n",
    "            (\"&Hat;\", \"^\"),\n",
    "            (\"&lowbar;\", \"_\"),\n",
    "            (\"&grave;\", \"`\"),\n",
    "            (\"&lcub;\", \"{\"),\n",
    "            (\"&verbar;\", \"|\"),\n",
    "            (\"&rcub;\", \"}\"),\n",
    "            # (\"\", \"\"),\n",
    "        ]\n",
    "        for (k, v) in html_entities:\n",
    "            x = str(x.replace(k, v))\n",
    "        return x\n",
    "\n",
    "    def remove_latex_and_code_tokens(tokens):\n",
    "        return [\n",
    "            x for x in tokens if not (x.startswith(\"$\") or x.startswith(\"\\\\\"))\n",
    "        ]\n",
    "\n",
    "    # Remove extra spaces\n",
    "    title = remove_html_special_symbols(\" \".join(\n",
    "        remove_latex_and_code_tokens(str(title).split()))).strip()\n",
    "    question = remove_html_special_symbols(\" \".join(\n",
    "        remove_latex_and_code_tokens(str(question).split()))).strip()\n",
    "    answer = remove_html_special_symbols(\" \".join(\n",
    "        remove_latex_and_code_tokens(str(answer).split()))).strip()\n",
    "\n",
    "    # Extract plain text from html\n",
    "    try:\n",
    "        soup_q = BeautifulSoup(question)\n",
    "        question = soup_q.get_text()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        soup_a = BeautifulSoup(answer)\n",
    "        answer = soup_a.get_text()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "\n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        \"[CLS] \" + title[:max_title_length] + \" [SEP] \" + question + \" [SEP]\",\n",
    "        None, 'longest_first', max_sequence_length)\n",
    "\n",
    "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "        \"[CLS] \" + answer + \" [SEP]\", None, 'longest_first',\n",
    "        max_sequence_length)\n",
    "\n",
    "    return [\n",
    "        input_ids_q, input_masks_q, input_segments_q,\n",
    "        input_ids_a, input_masks_a, input_segments_a\n",
    "    ]\n",
    "\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "\n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "        input_ids_a.append(ids_a)\n",
    "        input_masks_a.append(masks_a)\n",
    "        input_segments_a.append(segments_a)\n",
    "\n",
    "    return [\n",
    "        np.asarray(input_ids_q, dtype=np.int32),\n",
    "        np.asarray(input_masks_q, dtype=np.int32),\n",
    "        np.asarray(input_segments_q, dtype=np.int32),\n",
    "        np.asarray(input_ids_a, dtype=np.int32),\n",
    "        np.asarray(input_masks_a, dtype=np.int32),\n",
    "        np.asarray(input_segments_a, dtype=np.int32)\n",
    "    ]\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpearmanMonitorCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, valid_data, batch_size=16, fold=None):\n",
    "        self.valid_inputs = valid_data[0]\n",
    "        self.valid_outputs = valid_data[1]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valid_predictions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.valid_predictions.append(\n",
    "            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n",
    "\n",
    "        rho_val = compute_spearmanr_ignore_nan(\n",
    "            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n",
    "\n",
    "        print(f\" Fold {self.fold+1} Validation Score: {rho_val:.6f}\")\n",
    "        \n",
    "class SpearmanRhoEarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, valid_data, batch_size=16, fold=None, model_save_path=None, patience=2):\n",
    "        self.x_val = valid_data[0]\n",
    "        self.y_val = valid_data[1]\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "        self.model_save_path = model_save_path\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.current_best = -1\n",
    "        self.bad_epochs = 0\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred_val = self.model.predict(self.x_val, batch_size=self.batch_size)\n",
    "\n",
    "        rho_val = np.mean([spearmanr(\n",
    "            self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(\n",
    "                0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n",
    "\n",
    "        if rho_val >= self.current_best:\n",
    "            self.current_best = rho_val\n",
    "            # Save model\n",
    "            self.model.save_weights(self.model_save_path)\n",
    "        else:\n",
    "            self.bad_epochs += 1\n",
    "            print(f\"\\nEpoch {epoch}: no improvement\")\n",
    "            \n",
    "        if self.bad_epochs >= self.patience:\n",
    "            print(f\"\\nEpoch {epoch} early stopping ......\")\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "        print(f\"\\nFold {self.fold+1} Validation Score: {rho_val:.6f}\")\n",
    "        \n",
    "        return rho_val\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"`learner` contains essential learner utilities\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "class LRFinder:\n",
    "    \"\"\"\n",
    "    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n",
    "    See for details:\n",
    "    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.losses = []\n",
    "        self.lrs = []\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def on_batch_end(self, batch, logs, tolerance=4):\n",
    "        # Log the learning rate\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = logs['loss']\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        # Check whether the loss got too large or NaN\n",
    "        if math.isnan(loss) or loss > self.best_loss * tolerance:\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "\n",
    "        # Increase the learning rate for the next batch\n",
    "        lr *= self.lr_mult\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def find(self, x_train, y_train, start_lr=1e-6, end_lr=1e-1, batch_size=64, epochs=1, tolerance=4):\n",
    "        # num_batches = epochs * x_train.shape[0] / batch_size\n",
    "        num_batches = epochs * len(x_train) / batch_size\n",
    "        \n",
    "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(num_batches))\n",
    "\n",
    "        # Save weights into a file\n",
    "        self.model.save_weights('tmp.h5')\n",
    "\n",
    "        # Remember the original learning rate\n",
    "        original_lr = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        # Set the initial learning rate\n",
    "        K.set_value(self.model.optimizer.lr, start_lr)\n",
    "\n",
    "        callback = tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs, tolerance))\n",
    "\n",
    "        self.model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size, epochs=epochs,\n",
    "                        callbacks=[callback])\n",
    "\n",
    "        # Restore the weights to the state before model fitting\n",
    "        self.model.load_weights('tmp.h5')\n",
    "\n",
    "        # Restore the original learning rate\n",
    "        K.set_value(self.model.optimizer.lr, original_lr)\n",
    "\n",
    "    def find_generator(self, generator, start_lr=1e-6, end_lr=1e-1, epochs=1, tolerance=4, steps_per_epoch=None, **kw_fit):\n",
    "            if steps_per_epoch is None:\n",
    "                try:\n",
    "                    steps_per_epoch = len(generator)\n",
    "                except (ValueError, NotImplementedError) as e:\n",
    "                    raise e('`steps_per_epoch=None` is only valid for a'\n",
    "                            ' generator based on the '\n",
    "                            '`keras.utils.Sequence`'\n",
    "                            ' class. Please specify `steps_per_epoch` '\n",
    "                            'or use the `keras.utils.Sequence` class.')\n",
    "            self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(steps_per_epoch))\n",
    "\n",
    "            # Save weights into a file\n",
    "            self.model.save_weights('tmp.h5')\n",
    "\n",
    "            # Remember the original learning rate\n",
    "            original_lr = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "            # Set the initial learning rate\n",
    "            K.set_value(self.model.optimizer.lr, start_lr)\n",
    "\n",
    "            callback = tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch,\n",
    "                                      logs: self.on_batch_end(batch, logs, tolerance))\n",
    "\n",
    "            self.model.fit_generator(generator=generator,\n",
    "                                     epochs=epochs,\n",
    "                                     steps_per_epoch=steps_per_epoch,\n",
    "                                     callbacks=[callback],\n",
    "                                     **kw_fit)\n",
    "\n",
    "            # Restore the weights to the state before model fitting\n",
    "            self.model.load_weights('tmp.h5')\n",
    "\n",
    "            # Restore the original learning rate\n",
    "            K.set_value(self.model.optimizer.lr, original_lr)\n",
    "\n",
    "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5, log_scale=True):\n",
    "        \"\"\"\n",
    "        Plots the loss.\n",
    "        Parameters:\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "        \"\"\"\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
    "        if log_scale:\n",
    "            plt.xscale('log')\n",
    "\n",
    "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
    "        \"\"\"\n",
    "        Plots rate of change of the loss function.\n",
    "        Parameters:\n",
    "            sma - number of batches for simple moving average to smooth out the curve.\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "            y_lim - limits for the y axis.\n",
    "        \"\"\"\n",
    "        assert sma >= 1\n",
    "        derivatives = [0] * sma\n",
    "        for i in range(sma, len(self.lrs)):\n",
    "            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n",
    "            derivatives.append(derivative)\n",
    "\n",
    "        plt.ylabel(\"rate of loss change\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "        plt.ylim(y_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model Topology and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_tf = True\n",
    "pretrained_model_name = \"tf-roberta-base\"\n",
    "\n",
    "if is_tf:\n",
    "    model_class = TFAutoModel\n",
    "    tokenizer_class = AutoTokenizer\n",
    "else:\n",
    "    model_class = AutoModel\n",
    "    tokenizer_class = AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_class.from_pretrained(BERT_PATH +\n",
    "                                            f\"{pretrained_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "\n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "\n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "\n",
    "    pretrained_model = model_class.from_pretrained(BERT_PATH +\n",
    "                                                   f\"{pretrained_model_name}\")\n",
    "\n",
    "    # Get last hidden-state from 1st element of output\n",
    "    q_embedding = pretrained_model(q_id,\n",
    "                                   attention_mask=q_mask,\n",
    "                                   token_type_ids=q_atn)[0]\n",
    "    a_embedding = pretrained_model(a_id,\n",
    "                                   attention_mask=a_mask,\n",
    "                                   token_type_ids=a_atn)[0]\n",
    "\n",
    "    #     q_embedding = tf.keras.layers.SpatialDropout1D(embeddings_dropout)(q_embedding)\n",
    "    #     a_embedding = tf.keras.layers.SpatialDropout1D(embeddings_dropout)(a_embedding)\n",
    "\n",
    "    # Get CLS token output\n",
    "    q = q_embedding[:, 0, :]\n",
    "    a = a_embedding[:, 0, :]\n",
    "    #     q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    #     a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "\n",
    "    #     x = tf.keras.layers.Concatenate()([q, a])\n",
    "\n",
    "    #     x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    q = tf.keras.layers.Dense(256, activation='relu')(q)\n",
    "    a = tf.keras.layers.Dense(256, activation='relu')(a)\n",
    "\n",
    "\n",
    "    # Use sigmoid for multi-label predictions\n",
    "    q = tf.keras.layers.Dense(21, activation='sigmoid')(q)\n",
    "    a = tf.keras.layers.Dense(9, activation='sigmoid')(a)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([q, a])\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[\n",
    "        q_id,\n",
    "        q_mask,\n",
    "        q_atn,\n",
    "        a_id,\n",
    "        a_mask,\n",
    "        a_atn,\n",
    "    ],\n",
    "                                  outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arrays(df_train, input_categories, tokenizer,\n",
    "                              MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer,\n",
    "                                   MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split K-Folds by Unique Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(rand_seed)\n",
    "gkf = GroupKFold(n_splits=n_splits).split(X=df_train.question_body,\n",
    "                                          groups=df_train.question_body)\n",
    "gkf = list(gkf)\n",
    "len(gkf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "# tmp_model = create_model(pretrained_model)\n",
    "# tmp_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# tmp_model.compile(loss='binary_crossentropy', optimizer=tmp_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# finder = LRFinder(tmp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train_idx, valid_idx = list(gkf)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tmp_train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "# tmp_train_outputs = outputs[train_idx]\n",
    "# # tmp_valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "# # tmp_valid_outputs = outputs[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set_all_seeds(rand_seed)\n",
    "# finder.find(tmp_train_inputs, tmp_train_outputs,\n",
    "#             start_lr=1e-7, end_lr=9e-5,\n",
    "#             batch_size=4, epochs=5,\n",
    "#             tolerance=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# finder.plot_loss(log_scale=True, n_skip_beginning=5, n_skip_end=30)\n",
    "# finder.plot_loss(n_skip_beginning=10, n_skip_end=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# del tmp_model, tmp_optimizer, tmp_train_inputs, tmp_train_outputs, finder\n",
    "# del tmp_model, tmp_train_inputs, tmp_train_outputs, tmp_valid_inputs, tmp_valid_outputs, finder\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = \"exp_split_dense\"\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "infer_batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    set_all_seeds(rand_seed)\n",
    "\n",
    "    print(f\"Fine-tuning {pretrained_model_name} for Fold {fold+1} ......\")\n",
    "    SAVE_PATH = f\"{dataset_folder}/{pretrained_model_name}_{model_prefix}_fold{fold+1}.h5\"\n",
    "    # SAVE_PATH = f\"{dataset_folder}/{pretrained_model_name}_lr{learning_rate}_eps{epochs}_drop{dense_dropout}_fold{fold+1}.h5\"\n",
    "\n",
    "    train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "    train_outputs = outputs[train_idx]\n",
    "\n",
    "    valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "    valid_outputs = outputs[valid_idx]\n",
    "\n",
    "    K.clear_session()\n",
    "    model = create_model()\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    # Not implemented error in TF 2.0\n",
    "    # model = tf.keras.models.clone_model(template_model)\n",
    "\n",
    "    # Define callback to save the model\n",
    "    cbs = [\n",
    "        SpearmanRhoEarlyStoppingCallback(valid_data=(valid_inputs, valid_outputs),\n",
    "                                batch_size=infer_batch_size,\n",
    "                                fold=fold,\n",
    "                                model_save_path=SAVE_PATH,\n",
    "                                patience=early_stopping_patience),\n",
    "#         SpearmanMonitorCallback(valid_data=(valid_inputs, valid_outputs),\n",
    "#                                 batch_size=batch_size,\n",
    "#                                 fold=fold),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                             factor=0.1,\n",
    "                                             # factor=0.3,\n",
    "                                             min_delta=1e-4,\n",
    "                                             min_lr=1e-7,\n",
    "                                             patience=lr_decay_patience,\n",
    "                                             verbose=1),\n",
    "        # Save fine tuned model\n",
    "#         tf.keras.callbacks.ModelCheckpoint(filepath=SAVE_PATH,\n",
    "#                                            mode=\"min\",\n",
    "#                                            monitor=\"val_loss\",\n",
    "#                                            save_best_only=True,\n",
    "#                                            save_weights_only=True,\n",
    "#                                            verbose=1),\n",
    "#         tf.keras.callbacks.EarlyStopping(patience=early_stopping_patience,\n",
    "#                                          min_delta=1e-4,\n",
    "#                                          mode=\"min\",\n",
    "#                                          verbose=1)\n",
    "    ]\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    model.fit(train_inputs,\n",
    "              train_outputs,\n",
    "              validation_data=[valid_inputs, valid_outputs],\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              callbacks=cbs,\n",
    "              shuffle=True,\n",
    "              verbose=1)\n",
    "    # verbose=2)\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_weights(SAVE_PATH)\n",
    "\n",
    "    fold_val_preds = model.predict(valid_inputs, batch_size=infer_batch_size)\n",
    "\n",
    "    rho_val = compute_spearmanr_ignore_nan(valid_outputs, fold_val_preds)\n",
    "    print(f\"Fold {fold+1} Best Validation Score: {rho_val:.6f}\")\n",
    "\n",
    "    val_scores.append(rho_val)\n",
    "\n",
    "    del model, rho_val, fold_val_preds\n",
    "    gc.collect()\n",
    "\n",
    "    if debug_mode:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Validation Score: {np.mean(val_scores):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
