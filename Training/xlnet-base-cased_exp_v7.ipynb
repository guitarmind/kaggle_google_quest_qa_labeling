{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kernal Mode\n",
    "\n",
    "# !pip install -q ../input/tensorflow-determinism\n",
    "# !pip install -q ../input/huggingfacetokenizers/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl\n",
    "# !pip uninstall --yes pytorch-transformers\n",
    "# !pip install -q ../input/huggingface-transformers-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.1.0-rc0\n",
      "PyTorch version 1.1.0\n",
      "Transformers version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Use only title (100) + question (206) + answer (206) (failed)\n",
    "2. LR decay factor=0.3 (failed)\n",
    "3. Use one embedding input instead of two (failed)\n",
    "4. Use three embedding inputs instead of two (failed)\n",
    "5. Split question and anwer FC layers (good)\n",
    "7. Add category and domain as embeddings (good)\n",
    "8. Drop out=0.2 (failed)\n",
    "9. AdamW (failed)\n",
    "10. Cyclic LR (failed)\n",
    "11. Normallization for layer output (failed)\n",
    "12. netloc as feature\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import random, math, time\n",
    "import os, sys, re\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import bisect\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "# https://github.com/NVIDIA/tensorflow-determinism\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1' # TF 2.1\n",
    "# from tfdeterminism import patch\n",
    "# patch()\n",
    "\n",
    "import transformers\n",
    "from transformers import *\n",
    "\n",
    "import torch\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print('Tensorflow version', tf.__version__)\n",
    "\n",
    "print('PyTorch version', torch.__version__)\n",
    "\n",
    "print('Transformers version',\n",
    "      transformers.__version__)  # Current version: 2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_mode = True\n",
    "debug_mode = False\n",
    "\n",
    "kernel_mode = False\n",
    "# kernel_mode = True\n",
    "\n",
    "rand_seed = 20201120\n",
    "n_splits = 5\n",
    "\n",
    "dataset_folder = Path(\"/workspace/Kaggle/QA/\")\n",
    "BERT_PATH = \"/workspace/Kaggle/QA/pretrained_models/\"\n",
    "\n",
    "# dataset_folder = Path(\"../input/google-quest-challenge/\")\n",
    "# BERT_PATH = \"../input/huggingface-transformers/\"\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "# max_title_length = 50\n",
    "max_title_length = 100\n",
    "\n",
    "learning_rate = 2e-5\n",
    "# embeddings_dropout = 0.05\n",
    "# dense_dropout = 0.05\n",
    "# learning_rate = 2e-5\n",
    "embeddings_dropout = 0.2\n",
    "dense_dropout = 0.2\n",
    "\n",
    "if debug_mode:\n",
    "#     epochs = 2\n",
    "#     batch_size = 2\n",
    "    epochs = 15\n",
    "    batch_size = 2\n",
    "else:\n",
    "#     epochs = 6\n",
    "    epochs = 15\n",
    "    if kernel_mode:\n",
    "        batch_size = 4\n",
    "    else:\n",
    "        batch_size = 3\n",
    "#         batch_size = 4\n",
    "\n",
    "# lr_decay_patience = 1\n",
    "# early_stopping_patience = 2\n",
    "\n",
    "lr_decay_patience = 2\n",
    "early_stopping_patience = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6079, 41)\n",
      "Test shape: (476, 11)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(dataset_folder / 'train.csv')\n",
    "df_test = pd.read_csv(dataset_folder / 'test.csv')\n",
    "df_sub = pd.read_csv(dataset_folder / 'sample_submission.csv')\n",
    "print('Train shape:', df_train.shape)\n",
    "print('Test shape:', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output categories:\n",
      " ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
      "\n",
      "Input categories:\n",
      " ['question_title', 'question_body', 'answer']\n"
     ]
    }
   ],
   "source": [
    "output_categories = list(df_train.columns[11:])\n",
    "# Select only question title, body and answer\n",
    "input_categories = list(df_train.columns[[1, 2, 5]])\n",
    "\n",
    "print('\\nOutput categories:\\n', output_categories)\n",
    "print('\\nInput categories:\\n', input_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stackoverflow.com                      1253\n",
       "english.stackexchange.com               229\n",
       "superuser.com                           227\n",
       "electronics.stackexchange.com           221\n",
       "serverfault.com                         213\n",
       "                                       ... \n",
       "meta.christianity.stackexchange.com       4\n",
       "robotics.stackexchange.com                2\n",
       "meta.askubuntu.com                        2\n",
       "meta.math.stackexchange.com               2\n",
       "meta.codereview.stackexchange.com         2\n",
       "Name: host, Length: 63, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['host'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TECHNOLOGY       2441\n",
       "STACKOVERFLOW    1253\n",
       "CULTURE           963\n",
       "SCIENCE           713\n",
       "LIFE_ARTS         709\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['category'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract domain\n",
    "def extract_netloc(x):\n",
    "    tokens = x.split(\".\")\n",
    "    if len(tokens) > 3:\n",
    "        print(x)\n",
    "        return \".\".join(tokens[:2])\n",
    "        # looks like meta is a special site, we should keep it\n",
    "        # https://stackoverflow.com/help/whats-meta\n",
    "        # the part of the site where users discuss the workings and policies of Stack Overflow rather than discussing programming itself.\n",
    "        # return tokens[1]\n",
    "    else:\n",
    "        return tokens[0]\n",
    "\n",
    "\n",
    "# TODO: test it\n",
    "# df_train['netloc'] = df_train['host'].apply(\n",
    "#     lambda x: extract_netloc(x))\n",
    "# df_test['netloc'] = df_test['host'].apply(\n",
    "#     lambda x: extract_netloc(x))\n",
    "\n",
    "df_train['netloc'] = df_train['host'].apply(lambda x: x.split(\".\")[0])\n",
    "df_test['netloc'] = df_test['host'].apply(lambda x: x.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(rand_seed):\n",
    "    np.random.seed(rand_seed)\n",
    "    random.seed(rand_seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(rand_seed)\n",
    "    \n",
    "    # TF 2.0\n",
    "    tf.random.set_seed(rand_seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(rand_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    }
   ],
   "source": [
    "# Redirect outputs to console\n",
    "import sys\n",
    "jupyter_console = sys.stdout\n",
    "sys.stdout = open('/dev/stdout', 'w')\n",
    "\n",
    "# Append to log file\n",
    "# sys.stdout = open(f\"stdout.log\", 'a')\n",
    "# sys.stdout = jupyter_console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer,\n",
    "                                   max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "        inputs = tokenizer.encode_plus(str1,\n",
    "                                       str2,\n",
    "                                       add_special_tokens=True,\n",
    "                                       max_length=length,\n",
    "                                       truncation_strategy=truncation_strategy)\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "\n",
    "        return [input_ids, input_masks, input_segments]\n",
    "\n",
    "    def remove_html_special_symbols(x):\n",
    "        html_entities = [\n",
    "            (\"&quot;\", \"\\\"\"),\n",
    "            (\"&num;\", \"#\"),\n",
    "            (\"&dollar;\", \"$\"),\n",
    "            (\"&percnt;\", \"%\"),\n",
    "            (\"&amp;\", \"&\"),\n",
    "            (\"&apos;\", \"'\"),\n",
    "            (\"&lpar;\", \"(\"),\n",
    "            (\"&rpar;\", \")\"),\n",
    "            (\"&ast;\", \"*\"),\n",
    "            (\"&plus;\", \"+\"),\n",
    "            (\"&comma;\", \",\"),\n",
    "            (\"&minus;\", \"-\"),\n",
    "            (\"&period;\", \".\"),\n",
    "            (\"&sol;\", \"/\"),\n",
    "            (\"&colon;\", \":\"),\n",
    "            (\"&semi;\", \";\"),\n",
    "            (\"&lt;\", \"<\"),\n",
    "            (\"&equals;\", \"=\"),\n",
    "            (\"&gt;\", \">\"),\n",
    "            (\"&quest;\", \"?\"),\n",
    "            (\"&commat;\", \"@\"),\n",
    "            (\"&lsqb;\", \"[\"),\n",
    "            (\"&bsol;\", \"\\\\\"),\n",
    "            (\"&rsqb;\", \"]\"),\n",
    "            (\"&Hat;\", \"^\"),\n",
    "            (\"&lowbar;\", \"_\"),\n",
    "            (\"&grave;\", \"`\"),\n",
    "            (\"&lcub;\", \"{\"),\n",
    "            (\"&verbar;\", \"|\"),\n",
    "            (\"&rcub;\", \"}\"),\n",
    "            # (\"\", \"\"),\n",
    "        ]\n",
    "        for (k, v) in html_entities:\n",
    "            x = str(x.replace(k, v))\n",
    "        return x\n",
    "\n",
    "    def remove_latex_and_code_tokens(tokens):\n",
    "        return [\n",
    "            x for x in tokens if not (x.startswith(\"$\") or x.startswith(\"\\\\\"))\n",
    "        ]\n",
    "\n",
    "    # Remove extra spaces\n",
    "    title = remove_html_special_symbols(\" \".join(\n",
    "        remove_latex_and_code_tokens(str(title).split()))).strip()\n",
    "    question = remove_html_special_symbols(\" \".join(\n",
    "        remove_latex_and_code_tokens(str(question).split()))).strip()\n",
    "    answer = remove_html_special_symbols(\" \".join(\n",
    "        remove_latex_and_code_tokens(str(answer).split()))).strip()\n",
    "\n",
    "    # Extract plain text from html\n",
    "    try:\n",
    "        soup_q = BeautifulSoup(question)\n",
    "        question = soup_q.get_text()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        soup_a = BeautifulSoup(answer)\n",
    "        answer = soup_a.get_text()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        \"[CLS] \" + title[:max_title_length] + \" [SEP] \" + question + \" [SEP]\",\n",
    "        None, 'longest_first', max_sequence_length)\n",
    "\n",
    "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "        \"[CLS] \" + answer + \" [SEP]\", None, 'longest_first',\n",
    "        max_sequence_length)\n",
    "\n",
    "    return [\n",
    "        input_ids_q, input_masks_q, input_segments_q, input_ids_a,\n",
    "        input_masks_a, input_segments_a\n",
    "    ]\n",
    "\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "\n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "        input_ids_a.append(ids_a)\n",
    "        input_masks_a.append(masks_a)\n",
    "        input_segments_a.append(segments_a)\n",
    "\n",
    "    return [\n",
    "        np.asarray(input_ids_q, dtype=np.int32),\n",
    "        np.asarray(input_masks_q, dtype=np.int32),\n",
    "        np.asarray(input_segments_q, dtype=np.int32),\n",
    "        np.asarray(input_ids_a, dtype=np.int32),\n",
    "        np.asarray(input_masks_a, dtype=np.int32),\n",
    "        np.asarray(input_segments_a, dtype=np.int32)\n",
    "    ]\n",
    "\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpearmanMonitorCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, valid_data, batch_size=16, fold=None):\n",
    "        self.valid_inputs = valid_data[0]\n",
    "        self.valid_outputs = valid_data[1]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.valid_predictions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.valid_predictions.append(\n",
    "            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n",
    "\n",
    "        rho_val = compute_spearmanr_ignore_nan(\n",
    "            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n",
    "\n",
    "        print(f\" Fold {self.fold+1} Validation Score: {rho_val:.6f}\")\n",
    "        \n",
    "class SpearmanRhoEarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, valid_data, batch_size=16, fold=None, model_save_path=None, patience=2):\n",
    "        self.x_val = valid_data[0]\n",
    "        self.y_val = valid_data[1]\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "        self.model_save_path = model_save_path\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.current_best = -1\n",
    "        self.bad_epochs = 0\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred_val = self.model.predict(self.x_val, batch_size=self.batch_size)\n",
    "\n",
    "        rho_val = np.mean([spearmanr(\n",
    "            self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(\n",
    "                0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n",
    "\n",
    "        if rho_val >= self.current_best:\n",
    "            self.current_best = rho_val\n",
    "            # Save model\n",
    "            self.model.save_weights(self.model_save_path)\n",
    "        else:\n",
    "            self.bad_epochs += 1\n",
    "            print(f\"\\nEpoch {epoch}: no improvement\")\n",
    "            \n",
    "        if self.bad_epochs >= self.patience:\n",
    "            print(f\"\\nEpoch {epoch} early stopping ......\")\n",
    "            self.model.stop_training = True\n",
    "        \n",
    "        print(f\"\\nFold {self.fold+1} Validation Score: {rho_val:.6f}\")\n",
    "        \n",
    "        return rho_val\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model Topology and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_tf = True\n",
    "pretrained_model_name = \"xlnet-base-cased\"\n",
    "\n",
    "if is_tf:\n",
    "    model_class = TFAutoModel\n",
    "    tokenizer_class = AutoTokenizer\n",
    "else:\n",
    "    model_class = AutoModel\n",
    "    tokenizer_class = AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_class.from_pretrained(BERT_PATH +\n",
    "                                            f\"{pretrained_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embed_info):\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "\n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "\n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "\n",
    "    pretrained_model = model_class.from_pretrained(BERT_PATH +\n",
    "                                                   f\"{pretrained_model_name}\")\n",
    "\n",
    "    # Get last hidden-state from 1st element of output\n",
    "    q_embedding = pretrained_model(q_id,\n",
    "                                   attention_mask=q_mask,\n",
    "                                   token_type_ids=q_atn)[0]\n",
    "    a_embedding = pretrained_model(a_id,\n",
    "                                   attention_mask=a_mask,\n",
    "                                   token_type_ids=a_atn)[0]\n",
    "\n",
    "    # Get CLS token output\n",
    "    q = q_embedding[:, 0, :]\n",
    "    a = a_embedding[:, 0, :]\n",
    "\n",
    "    host_input = tf.keras.Input(shape=(1,), name=\"host_input\")\n",
    "    netloc_input = tf.keras.Input(shape=(1,), name=\"netloc_input\")\n",
    "    cate_input = tf.keras.Input(shape=(1,), name=\"category_input\")\n",
    "    \n",
    "    host_embed_info = embed_info[\"host\"]\n",
    "    host_embed = tf.keras.layers.Embedding(\n",
    "        input_dim=host_embed_info[0],\n",
    "        output_dim=host_embed_info[1],\n",
    "        input_length=(1, ))(host_input)\n",
    "\n",
    "    netloc_embed_info = embed_info[\"netloc\"]\n",
    "    netloc_embed = tf.keras.layers.Embedding(\n",
    "        input_dim=netloc_embed_info[0],\n",
    "        output_dim=netloc_embed_info[1],\n",
    "        input_length=(1, ))(netloc_input)\n",
    "\n",
    "    cate_embed_info = embed_info[\"category\"]\n",
    "    cate_embed = tf.keras.layers.Embedding(\n",
    "        input_dim=cate_embed_info[0],\n",
    "        output_dim=cate_embed_info[1],\n",
    "        input_length=(1, ))(cate_input)\n",
    "    \n",
    "    host_embed = tf.keras.layers.Reshape(target_shape=(host_embed_info[1],))(host_embed)\n",
    "    netloc_embed = tf.keras.layers.Reshape(target_shape=(netloc_embed_info[1],))(netloc_embed)\n",
    "    cate_embed = tf.keras.layers.Reshape(target_shape=(cate_embed_info[1],))(cate_embed)\n",
    "\n",
    "    # Batch normalization before concatenation\n",
    "    # q_pooler_output = tf.keras.layers.BatchNormalization(momentum=0.99)(q_pooler_output)\n",
    "    # a_pooler_output = tf.keras.layers.BatchNormalization(momentum=0.99)(a_pooler_output)\n",
    "\n",
    "    # q = tf.keras.layers.BatchNormalization(momentum=0.99)(q)\n",
    "    # host_embed = tf.keras.layers.BatchNormalization(momentum=0.99)(host_embed)\n",
    "    # cate_embed = tf.keras.layers.BatchNormalization(momentum=0.99)(cate_embed)\n",
    "\n",
    "    embed_concat = tf.keras.layers.Concatenate()([host_embed, netloc_embed, cate_embed])\n",
    "    embed_concat = tf.keras.layers.Dense(128, activation='relu')(embed_concat)\n",
    "\n",
    "    # Concatenation\n",
    "    q_concat = tf.keras.layers.Concatenate()([q, embed_concat])\n",
    "    # q_concat = tf.keras.layers.Concatenate()([q, host_embed, cate_embed, q_pooler_output])\n",
    "    q_concat = tf.keras.layers.Dense(256, activation='relu')(q_concat)\n",
    "\n",
    "    a_concat = tf.keras.layers.Concatenate()([a, embed_concat])\n",
    "    # a_concat = tf.keras.layers.Concatenate()([a, host_embed, cate_embed, a_pooler_output])\n",
    "    a_concat = tf.keras.layers.Dense(256, activation='relu')(a_concat)\n",
    "    \n",
    "    # Dense dropout\n",
    "    # q_concat = tf.keras.layers.Dropout(dense_dropout)(q_concat)\n",
    "    # a_concat = tf.keras.layers.Dropout(dense_dropout)(a_concat)\n",
    "\n",
    "    # Use sigmoid for multi-label predictions\n",
    "    q_concat = tf.keras.layers.Dense(21, activation='sigmoid')(q_concat)\n",
    "    a_concat = tf.keras.layers.Dense(9, activation='sigmoid')(a_concat)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([q_concat, a_concat])\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[\n",
    "        q_id,\n",
    "        q_mask,\n",
    "        q_atn,\n",
    "        a_id,\n",
    "        a_mask,\n",
    "        a_atn,\n",
    "        host_input,\n",
    "        netloc_input,\n",
    "        cate_input\n",
    "    ],\n",
    "                                  outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6079it [00:15, 399.37it/s]\n",
      "476it [00:01, 398.25it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arrays(df_train, input_categories, tokenizer,\n",
    "                              MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "test_inputs = compute_input_arrays(df_test, input_categories, tokenizer,\n",
    "                                   MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split K-Folds by Unique Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_all_seeds(rand_seed)\n",
    "gkf = GroupKFold(n_splits=n_splits).split(X=df_train.question_body,\n",
    "                                          groups=df_train.question_body)\n",
    "gkf = list(gkf)\n",
    "len(gkf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"`learner` contains essential learner utilities\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "class LRFinder:\n",
    "    \"\"\"\n",
    "    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n",
    "    See for details:\n",
    "    https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.losses = []\n",
    "        self.lrs = []\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def on_batch_end(self, batch, logs, tolerance=4):\n",
    "        # Log the learning rate\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # Log the loss\n",
    "        loss = logs['loss']\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        # Check whether the loss got too large or NaN\n",
    "        if math.isnan(loss) or loss > self.best_loss * tolerance:\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "\n",
    "        # Increase the learning rate for the next batch\n",
    "        lr *= self.lr_mult\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def find(self, x_train, y_train, start_lr=1e-6, end_lr=1e-1, batch_size=64, epochs=1, tolerance=4):\n",
    "        # num_batches = epochs * x_train.shape[0] / batch_size\n",
    "        num_batches = epochs * len(x_train) / batch_size\n",
    "        \n",
    "        self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(num_batches))\n",
    "\n",
    "        # Save weights into a file\n",
    "        self.model.save_weights('tmp.h5')\n",
    "\n",
    "        # Remember the original learning rate\n",
    "        original_lr = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        # Set the initial learning rate\n",
    "        K.set_value(self.model.optimizer.lr, start_lr)\n",
    "\n",
    "        callback = tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs, tolerance))\n",
    "\n",
    "        self.model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size, epochs=epochs,\n",
    "                        callbacks=[callback])\n",
    "\n",
    "        # Restore the weights to the state before model fitting\n",
    "        self.model.load_weights('tmp.h5')\n",
    "\n",
    "        # Restore the original learning rate\n",
    "        K.set_value(self.model.optimizer.lr, original_lr)\n",
    "\n",
    "    def find_generator(self, generator, start_lr=1e-6, end_lr=1e-1, epochs=1, tolerance=4, steps_per_epoch=None, **kw_fit):\n",
    "            if steps_per_epoch is None:\n",
    "                try:\n",
    "                    steps_per_epoch = len(generator)\n",
    "                except (ValueError, NotImplementedError) as e:\n",
    "                    raise e('`steps_per_epoch=None` is only valid for a'\n",
    "                            ' generator based on the '\n",
    "                            '`keras.utils.Sequence`'\n",
    "                            ' class. Please specify `steps_per_epoch` '\n",
    "                            'or use the `keras.utils.Sequence` class.')\n",
    "            self.lr_mult = (float(end_lr) / float(start_lr)) ** (float(1) / float(steps_per_epoch))\n",
    "\n",
    "            # Save weights into a file\n",
    "            self.model.save_weights('tmp.h5')\n",
    "\n",
    "            # Remember the original learning rate\n",
    "            original_lr = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "            # Set the initial learning rate\n",
    "            K.set_value(self.model.optimizer.lr, start_lr)\n",
    "\n",
    "            callback = tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch,\n",
    "                                      logs: self.on_batch_end(batch, logs, tolerance))\n",
    "\n",
    "            self.model.fit_generator(generator=generator,\n",
    "                                     epochs=epochs,\n",
    "                                     steps_per_epoch=steps_per_epoch,\n",
    "                                     callbacks=[callback],\n",
    "                                     **kw_fit)\n",
    "\n",
    "            # Restore the weights to the state before model fitting\n",
    "            self.model.load_weights('tmp.h5')\n",
    "\n",
    "            # Restore the original learning rate\n",
    "            K.set_value(self.model.optimizer.lr, original_lr)\n",
    "\n",
    "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5, log_scale=True):\n",
    "        \"\"\"\n",
    "        Plots the loss.\n",
    "        Parameters:\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "        \"\"\"\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
    "        if log_scale:\n",
    "            plt.xscale('log')\n",
    "\n",
    "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
    "        \"\"\"\n",
    "        Plots rate of change of the loss function.\n",
    "        Parameters:\n",
    "            sma - number of batches for simple moving average to smooth out the curve.\n",
    "            n_skip_beginning - number of batches to skip on the left.\n",
    "            n_skip_end - number of batches to skip on the right.\n",
    "            y_lim - limits for the y axis.\n",
    "        \"\"\"\n",
    "        assert sma >= 1\n",
    "        derivatives = [0] * sma\n",
    "        for i in range(sma, len(self.lrs)):\n",
    "            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n",
    "            derivatives.append(derivative)\n",
    "\n",
    "        plt.ylabel(\"rate of loss change\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "        plt.ylim(y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "# tmp_model = create_model(pretrained_model)\n",
    "# tmp_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# tmp_model.compile(loss='binary_crossentropy', optimizer=tmp_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# finder = LRFinder(tmp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train_idx, valid_idx = list(gkf)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# tmp_train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "# tmp_train_outputs = outputs[train_idx]\n",
    "# # tmp_valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "# # tmp_valid_outputs = outputs[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set_all_seeds(rand_seed)\n",
    "# finder.find(tmp_train_inputs, tmp_train_outputs,\n",
    "#             start_lr=1e-7, end_lr=9e-5,\n",
    "#             batch_size=4, epochs=5,\n",
    "#             tolerance=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# finder.plot_loss(log_scale=True, n_skip_beginning=5, n_skip_end=30)\n",
    "# finder.plot_loss(n_skip_beginning=10, n_skip_end=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2805"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del tmp_model, tmp_optimizer, tmp_train_inputs, tmp_train_outputs, finder\n",
    "# del tmp_model, tmp_train_inputs, tmp_train_outputs, tmp_valid_inputs, tmp_valid_outputs, finder\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = \"exp_cate_embed\"\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "infer_batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = []\n",
    "\n",
    "tf.executing_eagerly()\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "\n",
    "    set_all_seeds(rand_seed)\n",
    "\n",
    "    print(f\"Fine-tuning {pretrained_model_name} for Fold {fold+1} ......\")\n",
    "    SAVE_PATH = f\"{dataset_folder}/{pretrained_model_name}_{model_prefix}_fold{fold+1}.h5\"\n",
    "\n",
    "    train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "    train_outputs = outputs[train_idx]\n",
    "\n",
    "    valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "    valid_outputs = outputs[valid_idx]\n",
    "\n",
    "    # Extra categorical embeddings\n",
    "    embed_info = {}\n",
    "    category_features = {}\n",
    "\n",
    "    def extract_category_ids(train, test, c, info):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train[c])\n",
    "        # Set unknonwn category\n",
    "        test[c] = test[c].map(lambda s: '<unknown>'\n",
    "                              if s not in le.classes_ else s)\n",
    "\n",
    "        le_classes = le.classes_.tolist()\n",
    "        bisect.insort_left(le_classes, '<unknown>')\n",
    "        le.classes_ = le_classes\n",
    "\n",
    "        train[c + \"_label\"] = le.transform(train[c])\n",
    "        test[c + \"_label\"] = le.transform(test[c])\n",
    "\n",
    "        no_of_unique_cat = train[c + \"_label\"].nunique()\n",
    "        embedding_size = min(np.ceil((no_of_unique_cat) / 2), 50)\n",
    "        embedding_size = int(embedding_size)\n",
    "        vocab_size = no_of_unique_cat + 1\n",
    "        info[c] = (vocab_size, embedding_size)\n",
    "\n",
    "        print(f\"Extracted (vocab_size, embedding_size) for {c}: ({vocab_size}, {embedding_size})\")\n",
    "\n",
    "        return train[c + \"_label\"], test[c + \"_label\"]\n",
    "\n",
    "    host_train, host_val = extract_category_ids(df_train.iloc[train_idx, :].copy(),\n",
    "                                                df_train.iloc[valid_idx, :].copy(), \"host\",\n",
    "                                                embed_info)\n",
    "    netloc_train, netloc_val = extract_category_ids(df_train.iloc[train_idx, :].copy(),\n",
    "                                                df_train.iloc[valid_idx, :].copy(), \"netloc\",\n",
    "                                                embed_info)\n",
    "    cate_train, cate_val = extract_category_ids(df_train.iloc[train_idx, :].copy(),\n",
    "                                                df_train.iloc[valid_idx, :].copy(), \"category\",\n",
    "                                                embed_info)\n",
    "    \n",
    "    train_inputs.append(host_train)\n",
    "    train_inputs.append(netloc_train)\n",
    "    train_inputs.append(cate_train)\n",
    "    \n",
    "    valid_inputs.append(host_val)\n",
    "    valid_inputs.append(netloc_val)\n",
    "    valid_inputs.append(cate_val)\n",
    "\n",
    "    K.clear_session()\n",
    "    model = create_model(embed_info)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    # Define callback to save the model\n",
    "    cbs = [\n",
    "        SpearmanRhoEarlyStoppingCallback(valid_data=(valid_inputs,\n",
    "                                                     valid_outputs),\n",
    "                                         batch_size=infer_batch_size,\n",
    "                                         fold=fold,\n",
    "                                         model_save_path=SAVE_PATH,\n",
    "                                         patience=early_stopping_patience),\n",
    "        #         SpearmanMonitorCallback(valid_data=([valid_inputs, valid_outputs),\n",
    "        #                                 batch_size=batch_size,\n",
    "        #                                 fold=fold),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.1,\n",
    "            # factor=0.3,\n",
    "            min_delta=1e-4,\n",
    "            min_lr=1e-7,\n",
    "            patience=lr_decay_patience,\n",
    "            verbose=1),\n",
    "        # Save fine tuned model\n",
    "        #         tf.keras.callbacks.ModelCheckpoint(filepath=SAVE_PATH,\n",
    "        #                                            mode=\"min\",\n",
    "        #                                            monitor=\"val_loss\",\n",
    "        #                                            save_best_only=True,\n",
    "        #                                            save_weights_only=True,\n",
    "        #                                            verbose=1),\n",
    "        #         tf.keras.callbacks.EarlyStopping(patience=early_stopping_patience,\n",
    "        #                                          min_delta=1e-4,\n",
    "        #                                          mode=\"min\",\n",
    "        #                                          verbose=1)\n",
    "    ]\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#     optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    model.fit(train_inputs,\n",
    "              train_outputs,\n",
    "              validation_data=[valid_inputs,\n",
    "                               valid_outputs],\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              callbacks=cbs,\n",
    "              shuffle=True,\n",
    "              verbose=1)\n",
    "    # verbose=2)\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_weights(SAVE_PATH)\n",
    "\n",
    "    fold_val_preds = model.predict(valid_inputs, batch_size=infer_batch_size)\n",
    "\n",
    "    rho_val = compute_spearmanr_ignore_nan(valid_outputs, fold_val_preds)\n",
    "    print(f\"Fold {fold+1} Best Validation Score: {rho_val:.6f}\")\n",
    "\n",
    "    val_scores.append(rho_val)\n",
    "\n",
    "    del model, rho_val, fold_val_preds\n",
    "    gc.collect()\n",
    "\n",
    "    if debug_mode:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Validation Score: {np.mean(val_scores):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
